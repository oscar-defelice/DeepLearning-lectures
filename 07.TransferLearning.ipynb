{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49638680/159042792-8510fbd1-c4ac-4a48-8320-bc6c1a49cdae.png\">\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Transfer Learning](https://jamboard.google.com/d/1UmIdOUFZoUcjpPSOq_ryOyAjj5Houy87a--YxZSNi2g/edit?usp=sharing)\n",
    "\n",
    "\n",
    "_Transfer Learning_ is a technique allowing to improve performances on a learning task, having the result of another learning task.\n",
    "\n",
    "> It consists in substituting one or more layers in an already trained Neural Network in order to perform another task, while taking advantage of previous knowledge.\n",
    "\n",
    "## Strategies for Transfer Learning\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*9t7Po_ZFsT5_lZj445c-Lw.png)\n",
    "\n",
    "It might result in a great advantage both in performance and in computational time.\n",
    "We need to decide in which extent we want a previously trained network to be frozen. \n",
    "This depends, of course, on how much data we have to train our Machine Learning algorithm. \n",
    "\n",
    "Data availability             |  Transfer Learning strategy\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://miro.medium.com/max/552/1*heOde2iTazjgrF7YzvOFyQ.png)  |  ![](https://miro.medium.com/max/552/1*7ZD-u-h8hFPuN2PYJvLMBw.png)\n",
    "\n",
    "Usually transfer from task $A$ to task $B$ works fine when\n",
    "\n",
    "1. $A$ and $B$ have the same input $x$.\n",
    "2. We have a lot more data available for task $A$, rather than for task $B$.\n",
    "3. Low level features for $A$ can be useful to learn $B$.\n",
    "\n",
    "And now, questions of questions:\n",
    "\n",
    "> How to do this in Keras?\n",
    "\n",
    "In each layer object, one can call an optional argument `trainable`, by default set to `True`, and put it to `False` to freeze that layer.\n",
    "\n",
    "_Example_\n",
    "\n",
    "```python\n",
    "\n",
    "x_input = Input((n_features,), name='input_layer')\n",
    "x = Dense(units_1, name = 'layer_1', activation = 'relu', trainable = False)(x_input)\n",
    "x = Dense(units_2, name = 'layer_2', activation = 'softmax')(x)\n",
    "\n",
    "model = Model(input = x_input, output = x)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
