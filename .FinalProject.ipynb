{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYO1Zs3+UBxLp5PuEmyhd2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscar-defelice/DeepLearning-lectures/blob/master/FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Aq6rE2ri9v"
      },
      "source": [
        "# Text Classification problem\n",
        "\n",
        "Here we want to solve a famous text classification problem.\n",
        "We have the AG news dataset (available [here](https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz) or in the tensorflow dataset library).\n",
        "\n",
        "The main objectives are:\n",
        "1. Show a *brief* preliminary analysis of the data (classes are balanced, useful informations, feature selection, etc)\n",
        "2. Show some visualisation.\n",
        "3. Answer questions (later)\n",
        "4. Train a model with a test accuracy over the $80\\%$.\n",
        "5. *Optional* Deploy the model on a webpage through Tensorflow.js\n",
        "\n",
        "**Bonus**: make me learn something I did not know ðŸ™‚.\n",
        "\n",
        "#### Important note\n",
        "Any choice has to be properly explained and justified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkWoHEJ-s69z"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "We will use [ag_news dataset](https://www.tensorflow.org/datasets/catalog/ag_news_subset).\n",
        "\n",
        "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the complete dataset of 1 million of news. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
        "\n",
        "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n",
        "\n",
        "I suggest you to operate your preprocessing steps and then convert to a tensorflow dataset, which is the robust, and ready-to-parallel computing format you want to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKS4VOLrQJJ"
      },
      "source": [
        "%%bash\n",
        "wget https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz\n",
        "mkdir -p data && tar -xvzf ag_news_csv.tgz -C data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzvOdqamtbD5"
      },
      "source": [
        "## Hardware suggestion\n",
        "\n",
        "I strongly advice to work in colab, or any other environment with a GPU available in order to minimise training time and being able to run multiple model training. \n",
        "Recall that experimenting is crucial.\n",
        "\n",
        "To check whether your instance has a GPU activated you can run the following code\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:    \n",
        "    raise SystemError('GPU device not found')\n",
        "```\n",
        "\n",
        "If you do not have the GPU enabled, just go to:\n",
        "\n",
        "`Edit -> Notebook Settings -> Hardware accelerator -> Set to GPU`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebYFI3su3fz"
      },
      "source": [
        "### Questions to answer\n",
        "\n",
        "1. Is the dataset balanced?\n",
        "2. What kind of preprocessing you think is necessary?\n",
        "3. Can you use some sort of transfer learning? Which one?\n",
        "4. How many items contains the word \"*bush*\"?\n",
        "5. How many items containing the word \"*team*\" are classified as \"World news\"?\n",
        "6. How many items are classified as \"Science-Technology\" and do not contain the words \"phone\", \"computer\", \"technology\" and \"science\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVqk_vl8w6D4"
      },
      "source": [
        "## General assignements\n",
        "\n",
        "* Write your code following [PEP8 style guide](https://www.python.org/dev/peps/pep-0008/).\n",
        "* Docstrings has to be written in [Google Style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).\n",
        "* It is strongly adviced to write your modules to collect functions and import them in the notebook (this will make the following point almost effortless). To import custom modules in colab [look at this example](https://colab.research.google.com/drive/1uvHuizCBqFgvbCwEhK7FvU8JW0AfxgJw#scrollTo=psH0aLrvoh78).\n",
        "* Once you are sure the notebook runs smoothly, write a python script to be exectued from a command line interpreter to train your model:\n",
        "```bash\n",
        "python3 -m train -conf=config.yml\n",
        "```\n",
        "The `config.yml` file has to contain configuration instructions on model architecture (kind of layers, number of layers, number of units, etc.), on training (number of epochs, size of batches, if apply early stopping, optimiser, etc.) and on script metadata (where to get data, where to save output model).\n",
        "\n",
        "* Finally (optionally), you can serve your model on a webpage thanks to tensorflow.js."
      ]
    }
  ]
}
