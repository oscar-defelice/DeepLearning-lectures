{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1200/0*3I4P4pkL1xySQS9B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Structure a Machine Learning project\n",
    "\n",
    "Here we study the best strategies to work on a big problem in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://jamboard.google.com/d/1z45e4QmQ0iZAVbgoVi4QxD7DjhW9sX8i2-1_TKKkmlo/edit?usp=sharing) a brief case of study for this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Parameter Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.researchgate.net/publication/307087929/figure/fig6/AS:399685689856008@1472303902570/For-cross-validation-and-cross-testing-data-are-divided-into-two-separate-sets-only.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous discussion, we left the validation set a bit apart. This is the moment to cope with it.\n",
    "\n",
    "The train/test split may introduce an error, due to the fact that we may exclude data that are crucial for the algorithm. For example, think about a binary classification problem, in the case the slit completely excludes a class.\n",
    "\n",
    "This will result in overfitting, even though we’re trying to avoid it! This is where cross validation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid this, we can perform something called __cross validation__. It is very similar to train/test split, but it is applied to more subsets. Meaning, we split our data into $k$ subsets, and train on $k-1$ one of those subset. What we do is to hold the last subset for test. We’re able to do it for each of the subsets.\n",
    "\n",
    "There are several cross validation methods, we are going to go over two of them: the first is _K-Folds Cross Validation_ and the second is _Leave One Out Cross Validation_ (LOOCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $K$-Folds Cross Validation we split our data into $k$ different subsets (or folds). We use $k-1$ subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1400/1*J2B_bcbd1-s1kpWOu_FZrg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a concrete idea about how this works, we take an example directly from [sklearn documentation for $k$-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array\n",
    "y = np.array([1, 2, 3, 4]) # Create another array\n",
    "\n",
    "kf = KFold(n_splits=3) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can print out the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1 3] TEST: [2]\n",
      "TRAIN: [0 1 2] TEST: [3]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the function split the original data into different subsets of the data. \n",
    "This is a very simple example, but it explains the concept pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method we want to analyse is the so-called [Leave One Out Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of cross validation, the number of folds (subsets) equals to the number of observations we have in the dataset. We then average ALL of these folds and build our model with the average. We then test the model against the last fold. Because we would get a big number of training sets (equals to the number of samples), this method is very computationally expensive and should be used on small datasets. If the dataset is big, it would most likely be better to use a different method, like $k$-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's take as example the one from [`sklearn` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut # Import LeaveOneOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 2 3] TEST: [0]\n",
      "X_train:  [[3 4]\n",
      " [5 6]\n",
      " [7 8]] X_test:  [[1 2]] y_train:  [2 3 4] y_test:  [1]\n",
      "=========================\n",
      "TRAIN: [0 2 3] TEST: [1]\n",
      "X_train:  [[1 2]\n",
      " [5 6]\n",
      " [7 8]] X_test:  [[3 4]] y_train:  [1 3 4] y_test:  [2]\n",
      "=========================\n",
      "TRAIN: [0 1 3] TEST: [2]\n",
      "X_train:  [[1 2]\n",
      " [3 4]\n",
      " [7 8]] X_test:  [[5 6]] y_train:  [1 2 4] y_test:  [3]\n",
      "=========================\n",
      "TRAIN: [0 1 2] TEST: [3]\n",
      "X_train:  [[1 2]\n",
      " [3 4]\n",
      " [5 6]] X_test:  [[7 8]] y_train:  [1 2 3] y_test:  [4]\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print('X_train: ', X_train, 'X_test: ', X_test, 'y_train: ', y_train, 'y_test: ', y_test)\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Cross Validation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We presented two of the most used approaches to cross validation. However, one can check further methods on the [`sklearn` documentation webpage](https://scikit-learn.org/stable/modules/classes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the known and loved iris dataset to build a neural network classifier. We will make use of cross-validation to choose hyperparameter values:\n",
    "1. how many layers\n",
    "2. how many hidden units\n",
    "3. dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 2.1451 - accuracy: 0.3922 - val_loss: 1.6535 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "102/102 [==============================] - 0s 193us/step - loss: 1.6105 - accuracy: 0.4118 - val_loss: 1.0757 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "102/102 [==============================] - 0s 204us/step - loss: 1.3158 - accuracy: 0.4020 - val_loss: 0.8998 - val_accuracy: 0.5385\n",
      "Epoch 4/20\n",
      "102/102 [==============================] - 0s 187us/step - loss: 1.0549 - accuracy: 0.4804 - val_loss: 0.8843 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 1.0312 - accuracy: 0.5000 - val_loss: 0.9490 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "102/102 [==============================] - 0s 163us/step - loss: 0.9738 - accuracy: 0.5294 - val_loss: 0.9812 - val_accuracy: 0.0385\n",
      "Epoch 7/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 0.9079 - accuracy: 0.6078 - val_loss: 1.0275 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.8384 - accuracy: 0.6863 - val_loss: 1.1014 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 0.8291 - accuracy: 0.6176 - val_loss: 1.1470 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.8296 - accuracy: 0.6078 - val_loss: 1.1764 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 0.8081 - accuracy: 0.6176 - val_loss: 1.1708 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.7215 - accuracy: 0.6863 - val_loss: 1.1572 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.7037 - accuracy: 0.7353 - val_loss: 1.1277 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.6616 - accuracy: 0.7451 - val_loss: 1.0742 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "102/102 [==============================] - 0s 247us/step - loss: 0.6299 - accuracy: 0.7353 - val_loss: 1.0179 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "102/102 [==============================] - 0s 158us/step - loss: 0.6063 - accuracy: 0.6863 - val_loss: 1.0365 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "102/102 [==============================] - 0s 167us/step - loss: 0.5606 - accuracy: 0.8137 - val_loss: 0.9844 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "102/102 [==============================] - 0s 166us/step - loss: 0.5947 - accuracy: 0.7647 - val_loss: 0.9974 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "102/102 [==============================] - 0s 167us/step - loss: 0.5291 - accuracy: 0.8039 - val_loss: 0.8973 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "102/102 [==============================] - 0s 170us/step - loss: 0.5183 - accuracy: 0.8431 - val_loss: 0.9271 - val_accuracy: 0.0000e+00\n",
      "Score for fold 1: loss of 0.5198850035667419; accuracy of 68.18181872367859%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 1.4297 - accuracy: 0.5686 - val_loss: 2.8435 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "102/102 [==============================] - 0s 186us/step - loss: 1.0637 - accuracy: 0.6667 - val_loss: 1.5069 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "102/102 [==============================] - 0s 183us/step - loss: 0.9266 - accuracy: 0.5980 - val_loss: 1.2207 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "102/102 [==============================] - 0s 183us/step - loss: 0.8709 - accuracy: 0.6275 - val_loss: 1.4778 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "102/102 [==============================] - 0s 183us/step - loss: 0.7516 - accuracy: 0.6765 - val_loss: 1.2289 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "102/102 [==============================] - 0s 179us/step - loss: 0.6622 - accuracy: 0.7059 - val_loss: 0.9343 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "102/102 [==============================] - 0s 173us/step - loss: 0.5826 - accuracy: 0.7745 - val_loss: 0.7394 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "102/102 [==============================] - 0s 185us/step - loss: 0.6643 - accuracy: 0.6765 - val_loss: 0.9549 - val_accuracy: 0.0385\n",
      "Epoch 9/20\n",
      "102/102 [==============================] - 0s 168us/step - loss: 0.5706 - accuracy: 0.7745 - val_loss: 0.9278 - val_accuracy: 0.1154\n",
      "Epoch 10/20\n",
      "102/102 [==============================] - 0s 163us/step - loss: 0.5331 - accuracy: 0.8137 - val_loss: 0.7755 - val_accuracy: 0.3462\n",
      "Epoch 11/20\n",
      "102/102 [==============================] - 0s 156us/step - loss: 0.4394 - accuracy: 0.8627 - val_loss: 0.6441 - val_accuracy: 0.6154\n",
      "Epoch 12/20\n",
      "102/102 [==============================] - 0s 160us/step - loss: 0.4570 - accuracy: 0.8333 - val_loss: 0.6432 - val_accuracy: 0.6154\n",
      "Epoch 13/20\n",
      "102/102 [==============================] - 0s 161us/step - loss: 0.4279 - accuracy: 0.8431 - val_loss: 0.6560 - val_accuracy: 0.5769\n",
      "Epoch 14/20\n",
      "102/102 [==============================] - 0s 160us/step - loss: 0.4456 - accuracy: 0.7941 - val_loss: 0.6536 - val_accuracy: 0.5769\n",
      "Epoch 15/20\n",
      "102/102 [==============================] - 0s 166us/step - loss: 0.3685 - accuracy: 0.8725 - val_loss: 0.6407 - val_accuracy: 0.5769\n",
      "Epoch 16/20\n",
      "102/102 [==============================] - 0s 156us/step - loss: 0.3961 - accuracy: 0.8431 - val_loss: 0.8366 - val_accuracy: 0.2692\n",
      "Epoch 17/20\n",
      "102/102 [==============================] - 0s 163us/step - loss: 0.3662 - accuracy: 0.8725 - val_loss: 0.7542 - val_accuracy: 0.4231\n",
      "Epoch 18/20\n",
      "102/102 [==============================] - 0s 160us/step - loss: 0.3228 - accuracy: 0.8824 - val_loss: 0.5966 - val_accuracy: 0.6538\n",
      "Epoch 19/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.3702 - accuracy: 0.8431 - val_loss: 0.6335 - val_accuracy: 0.6154\n",
      "Epoch 20/20\n",
      "102/102 [==============================] - 0s 163us/step - loss: 0.3574 - accuracy: 0.8431 - val_loss: 0.8352 - val_accuracy: 0.4231\n",
      "Score for fold 2: loss of 0.1772056519985199; accuracy of 100.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 1.9199 - accuracy: 0.3922 - val_loss: 0.9148 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "102/102 [==============================] - 0s 215us/step - loss: 1.0661 - accuracy: 0.5196 - val_loss: 1.1539 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 0.8963 - accuracy: 0.6373 - val_loss: 1.2031 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.7967 - accuracy: 0.6569 - val_loss: 1.4655 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "102/102 [==============================] - 0s 164us/step - loss: 0.7249 - accuracy: 0.7353 - val_loss: 1.6955 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "102/102 [==============================] - 0s 173us/step - loss: 0.6512 - accuracy: 0.8137 - val_loss: 1.7433 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "102/102 [==============================] - 0s 173us/step - loss: 0.5781 - accuracy: 0.7843 - val_loss: 1.8392 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "102/102 [==============================] - 0s 174us/step - loss: 0.6056 - accuracy: 0.8039 - val_loss: 1.9630 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "102/102 [==============================] - 0s 172us/step - loss: 0.5261 - accuracy: 0.8431 - val_loss: 2.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "102/102 [==============================] - 0s 168us/step - loss: 0.5473 - accuracy: 0.8333 - val_loss: 1.8535 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "102/102 [==============================] - 0s 165us/step - loss: 0.4690 - accuracy: 0.8431 - val_loss: 1.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "102/102 [==============================] - 0s 187us/step - loss: 0.4894 - accuracy: 0.8235 - val_loss: 1.4739 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "102/102 [==============================] - 0s 188us/step - loss: 0.4345 - accuracy: 0.8529 - val_loss: 1.3969 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "102/102 [==============================] - 0s 181us/step - loss: 0.4299 - accuracy: 0.8529 - val_loss: 1.3199 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "102/102 [==============================] - 0s 194us/step - loss: 0.4171 - accuracy: 0.8529 - val_loss: 1.2669 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "102/102 [==============================] - 0s 178us/step - loss: 0.4265 - accuracy: 0.8333 - val_loss: 1.2888 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "102/102 [==============================] - 0s 180us/step - loss: 0.4080 - accuracy: 0.8529 - val_loss: 1.1922 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "102/102 [==============================] - 0s 181us/step - loss: 0.3971 - accuracy: 0.8431 - val_loss: 0.9717 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "102/102 [==============================] - 0s 204us/step - loss: 0.3611 - accuracy: 0.8824 - val_loss: 1.0332 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "102/102 [==============================] - 0s 180us/step - loss: 0.4049 - accuracy: 0.8039 - val_loss: 1.1930 - val_accuracy: 0.0000e+00\n",
      "Score for fold 3: loss of 0.5818617343902588; accuracy of 59.090906381607056%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1456 - accuracy: 0.4563 - val_loss: 1.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 0s 182us/step - loss: 0.9503 - accuracy: 0.5534 - val_loss: 1.7846 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 0s 182us/step - loss: 0.7594 - accuracy: 0.7573 - val_loss: 1.3284 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 0s 165us/step - loss: 0.6544 - accuracy: 0.6990 - val_loss: 0.6057 - val_accuracy: 0.8462\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 0s 177us/step - loss: 0.6003 - accuracy: 0.7476 - val_loss: 0.8407 - val_accuracy: 0.3462\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 0s 172us/step - loss: 0.5538 - accuracy: 0.7864 - val_loss: 0.8374 - val_accuracy: 0.3462\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 0s 166us/step - loss: 0.4783 - accuracy: 0.8058 - val_loss: 0.5969 - val_accuracy: 0.8462\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 0s 173us/step - loss: 0.5063 - accuracy: 0.8058 - val_loss: 1.0107 - val_accuracy: 0.0385\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 0s 168us/step - loss: 0.4060 - accuracy: 0.8641 - val_loss: 0.8986 - val_accuracy: 0.2308\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 0s 165us/step - loss: 0.4340 - accuracy: 0.8447 - val_loss: 0.7334 - val_accuracy: 0.3846\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 0s 164us/step - loss: 0.4488 - accuracy: 0.8350 - val_loss: 0.7879 - val_accuracy: 0.3846\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 0s 164us/step - loss: 0.4161 - accuracy: 0.8350 - val_loss: 0.6995 - val_accuracy: 0.4231\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 0s 157us/step - loss: 0.3468 - accuracy: 0.8544 - val_loss: 0.5064 - val_accuracy: 0.9231\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 0s 157us/step - loss: 0.3721 - accuracy: 0.8447 - val_loss: 0.6486 - val_accuracy: 0.6154\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 0s 211us/step - loss: 0.3167 - accuracy: 0.9320 - val_loss: 0.8643 - val_accuracy: 0.3077\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 0s 162us/step - loss: 0.3496 - accuracy: 0.8738 - val_loss: 0.7349 - val_accuracy: 0.4231\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 0s 175us/step - loss: 0.3424 - accuracy: 0.8641 - val_loss: 0.6338 - val_accuracy: 0.6538\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 0s 166us/step - loss: 0.2780 - accuracy: 0.9029 - val_loss: 0.5376 - val_accuracy: 0.8462\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 0s 161us/step - loss: 0.3180 - accuracy: 0.8641 - val_loss: 0.6707 - val_accuracy: 0.5385\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 0s 163us/step - loss: 0.2986 - accuracy: 0.8932 - val_loss: 0.6145 - val_accuracy: 0.6154\n",
      "Score for fold 4: loss of 0.2763393521308899; accuracy of 100.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0318 - accuracy: 0.4660 - val_loss: 1.1946 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 0s 188us/step - loss: 0.9361 - accuracy: 0.4854 - val_loss: 1.2412 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 0s 182us/step - loss: 0.8834 - accuracy: 0.6019 - val_loss: 1.1857 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 0s 183us/step - loss: 0.8535 - accuracy: 0.5922 - val_loss: 1.1921 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 0s 183us/step - loss: 0.7379 - accuracy: 0.7379 - val_loss: 1.1029 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 0s 210us/step - loss: 0.7032 - accuracy: 0.6796 - val_loss: 0.9239 - val_accuracy: 0.1538\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 0s 175us/step - loss: 0.6326 - accuracy: 0.7573 - val_loss: 0.8256 - val_accuracy: 0.4231\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 0s 163us/step - loss: 0.6018 - accuracy: 0.7573 - val_loss: 0.8448 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 0s 111us/step - loss: 0.5226 - accuracy: 0.8252 - val_loss: 0.9006 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 0s 110us/step - loss: 0.5694 - accuracy: 0.7961 - val_loss: 0.8398 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 0s 110us/step - loss: 0.4479 - accuracy: 0.8252 - val_loss: 0.8621 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 0s 112us/step - loss: 0.4219 - accuracy: 0.8155 - val_loss: 0.9895 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 0s 120us/step - loss: 0.4183 - accuracy: 0.8058 - val_loss: 0.9332 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 0s 136us/step - loss: 0.4236 - accuracy: 0.8155 - val_loss: 0.8619 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 0s 120us/step - loss: 0.4042 - accuracy: 0.8350 - val_loss: 0.9067 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 0s 165us/step - loss: 0.4052 - accuracy: 0.8544 - val_loss: 0.8949 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 0s 165us/step - loss: 0.4035 - accuracy: 0.8155 - val_loss: 0.8684 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 0s 169us/step - loss: 0.3760 - accuracy: 0.8447 - val_loss: 0.9232 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 0s 168us/step - loss: 0.3577 - accuracy: 0.8738 - val_loss: 0.9935 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 0s 174us/step - loss: 0.3961 - accuracy: 0.7961 - val_loss: 0.7926 - val_accuracy: 0.1923\n",
      "Score for fold 5: loss of 0.41321060061454773; accuracy of 80.95238208770752%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2671 - accuracy: 0.3592 - val_loss: 1.1119 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 0s 194us/step - loss: 1.0159 - accuracy: 0.5340 - val_loss: 1.0266 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 0s 158us/step - loss: 0.9033 - accuracy: 0.7573 - val_loss: 1.0274 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 0s 160us/step - loss: 0.7867 - accuracy: 0.7767 - val_loss: 1.2872 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 0s 159us/step - loss: 0.6741 - accuracy: 0.8350 - val_loss: 1.3228 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 0s 152us/step - loss: 0.5771 - accuracy: 0.8447 - val_loss: 1.2756 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 0s 189us/step - loss: 0.5980 - accuracy: 0.8447 - val_loss: 1.2531 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 0s 149us/step - loss: 0.5151 - accuracy: 0.8544 - val_loss: 1.1224 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 0s 148us/step - loss: 0.4572 - accuracy: 0.8544 - val_loss: 1.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 0s 146us/step - loss: 0.4415 - accuracy: 0.8544 - val_loss: 1.0705 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 0s 140us/step - loss: 0.4827 - accuracy: 0.8447 - val_loss: 1.1466 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 0s 122us/step - loss: 0.4093 - accuracy: 0.8641 - val_loss: 1.1210 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 0s 131us/step - loss: 0.3839 - accuracy: 0.8641 - val_loss: 1.1568 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 0s 144us/step - loss: 0.3865 - accuracy: 0.8252 - val_loss: 1.1690 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 0s 171us/step - loss: 0.3629 - accuracy: 0.8641 - val_loss: 1.1117 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 0s 207us/step - loss: 0.3577 - accuracy: 0.8641 - val_loss: 1.0386 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 0s 187us/step - loss: 0.3426 - accuracy: 0.8641 - val_loss: 1.0898 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 0s 192us/step - loss: 0.3594 - accuracy: 0.8641 - val_loss: 0.9287 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 0s 187us/step - loss: 0.3333 - accuracy: 0.8738 - val_loss: 0.9615 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 0s 182us/step - loss: 0.3564 - accuracy: 0.8544 - val_loss: 1.0310 - val_accuracy: 0.0000e+00\n",
      "Score for fold 6: loss of 0.5358168482780457; accuracy of 52.3809552192688%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3961 - accuracy: 0.3398 - val_loss: 1.3222 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 0s 203us/step - loss: 0.9971 - accuracy: 0.5825 - val_loss: 1.4517 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 0s 167us/step - loss: 0.8994 - accuracy: 0.7184 - val_loss: 1.3856 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 0s 251us/step - loss: 0.8544 - accuracy: 0.6990 - val_loss: 1.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 0s 158us/step - loss: 0.8160 - accuracy: 0.7573 - val_loss: 1.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 0s 157us/step - loss: 0.7454 - accuracy: 0.7864 - val_loss: 1.6357 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 0s 152us/step - loss: 0.6398 - accuracy: 0.8155 - val_loss: 1.5905 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 0s 123us/step - loss: 0.6035 - accuracy: 0.8058 - val_loss: 1.5554 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 0s 124us/step - loss: 0.5526 - accuracy: 0.7864 - val_loss: 1.3572 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 0s 123us/step - loss: 0.4360 - accuracy: 0.8155 - val_loss: 1.2176 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 0s 124us/step - loss: 0.3998 - accuracy: 0.8155 - val_loss: 1.0566 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 0s 125us/step - loss: 0.3853 - accuracy: 0.7864 - val_loss: 0.8109 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 0s 122us/step - loss: 0.3411 - accuracy: 0.8447 - val_loss: 1.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 0s 123us/step - loss: 0.3593 - accuracy: 0.8155 - val_loss: 1.2650 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 0s 122us/step - loss: 0.3391 - accuracy: 0.8447 - val_loss: 0.8404 - val_accuracy: 0.2692\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 0s 122us/step - loss: 0.3207 - accuracy: 0.8738 - val_loss: 0.7998 - val_accuracy: 0.3462\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 0s 134us/step - loss: 0.2723 - accuracy: 0.8932 - val_loss: 0.8801 - val_accuracy: 0.2692\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 0s 160us/step - loss: 0.2848 - accuracy: 0.9126 - val_loss: 1.0243 - val_accuracy: 0.0385\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 0s 158us/step - loss: 0.2834 - accuracy: 0.9223 - val_loss: 0.6500 - val_accuracy: 0.6154\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 0s 156us/step - loss: 0.2348 - accuracy: 0.9223 - val_loss: 0.9426 - val_accuracy: 0.2308\n",
      "Score for fold 7: loss of 0.3221375644207001; accuracy of 71.42857313156128%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5198850035667419 - Accuracy: 68.18181872367859%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1772056519985199 - Accuracy: 100.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5818617343902588 - Accuracy: 59.090906381607056%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2763393521308899 - Accuracy: 100.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.41321060061454773 - Accuracy: 80.95238208770752%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.5358168482780457 - Accuracy: 52.3809552192688%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.3221375644207001 - Accuracy: 71.42857313156128%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 76.00494793483189 (+- 17.330091675632964)\n",
      "> Loss: 0.403779536485672\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# Model configuration\n",
    "batch_size = 10 \n",
    "loss_function = categorical_crossentropy\n",
    "units = None\n",
    "n_classes = 3 # Fixed: the different Iris flowers\n",
    "n_epochs = None\n",
    "n_folds = 7\n",
    "drop_rate = None \n",
    "optimizer = Adam()\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "# Load Iris data\n",
    "X=load_iris().data\n",
    "Y=load_iris().target\n",
    "\n",
    "# Encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# Convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y = to_categorical(encoded_Y)\n",
    "\n",
    "## Train-Test split\n",
    "#X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Determine shape of the data\n",
    "input_shape = X.shape[1]\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold = 1\n",
    "for train, test in kfold.split(X, Y):\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim = input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss_function,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(X[train], Y[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=n_epochs,\n",
    "              verbose=verbosity,\n",
    "              validation_split=validation_split)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold += 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Change hyperparameters and check accuracy scores.\n",
    "2. (Harder) Write a script searching for the best hyperparameter configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint for exercise 2\n",
    "\n",
    "Think about the hyperparameter search.\n",
    "Which kind of approach would you choose? A grid search or an exploration on a random set of points?\n",
    "\n",
    "[Answer here](https://analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/#:~:text=One%20of%20the%20drawbacks%20of,aliasing%20around%20the%20right%20set.). Try to answer on your own before open the link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lectures",
   "language": "python",
   "name": "lectures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
