{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sol.FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscar-defelice/DeepLearning-lectures/blob/master/FinalProject_proposedSolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Aq6rE2ri9v"
      },
      "source": [
        "# Text Classification problem\n",
        "\n",
        "Here we want to solve a famous text classification problem.\n",
        "We have the Sentiment 140 Twitter dataset (available [here](https://www.tensorflow.org/datasets/catalog/sentiment140) or in the tensorflow dataset library).\n",
        "\n",
        "The main objectives are:\n",
        "1. Show a *brief* preliminary analysis of the data (classes are balanced, useful informations, feature selection, etc)\n",
        "2. Show some visualisation.\n",
        "3. Answer questions (later)\n",
        "4. Train a model with a test accuracy over the $80\\%$.\n",
        "5. *Optional* Deploy the model on a webpage through Tensorflow.js\n",
        "\n",
        "**Bonus**: make me learn something I did not know ðŸ™‚.\n",
        "\n",
        "#### Important note\n",
        "Any choice has to be properly explained and justified.\n",
        "\n",
        "<details>\n",
        "    <summary><b>HINT</b></summary> \n",
        "    \n",
        "    Make use of open-source implementations of similar problems you can easily find online!\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkWoHEJ-s69z"
      },
      "source": [
        "## The dataset\n",
        "<details>\n",
        "    <summary><b>Click to Expand</b></summary> \n",
        "    \n",
        "We will use [twitter_sentiment dataset](https://www.tensorflow.org/datasets/catalog/sentiment140).\n",
        "\n",
        "### What is Sentiment140?\n",
        "\n",
        "Sentiment140 allows you to discover the sentiment of a brand, product, or topic on Twitter.\n",
        "\n",
        "### How does this work?\n",
        "You can read about our approach in our technical report: [Twitter Sentiment Classification](http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf) using Distant Supervision. There are also additional features that are not described in this paper.\n",
        "\n",
        "### Who created this?\n",
        "Sentiment140 was created by Alec Go, Richa Bhayani, and Lei Huang, who were Computer Science graduate students at Stanford University.\n",
        "\n",
        "    \n",
        "**Note**: you can directly download the dataset from [tensorflow datasets](https://www.tensorflow.org/datasets/catalog/sentiment140).\n",
        "</details>\n",
        "I suggest you to operate your preprocessing steps and then convert to a tensorflow dataset, which is the robust, and ready-to-parallel computing format you want to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKS4VOLrQJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563a02a1-b45a-42ad-ae01-50fb3cf5efed"
      },
      "source": [
        "# Make data directory if it doesn't exist\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "!unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 09:26:30--  https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip\n",
            "Resolving nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)... 162.243.189.2\n",
            "Connecting to nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)|162.243.189.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85088192 (81M) [application/zip]\n",
            "Saving to: â€˜data/training.1600000.processed.noemoticon.csv.zipâ€™\n",
            "\n",
            "training.1600000.pr 100%[===================>]  81.15M  14.3MB/s    in 7.5s    \n",
            "\n",
            "2021-05-26 09:26:38 (10.9 MB/s) - â€˜data/training.1600000.processed.noemoticon.csv.zipâ€™ saved [85088192/85088192]\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "  inflating: data/training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzvOdqamtbD5"
      },
      "source": [
        "## Hardware suggestion\n",
        "\n",
        "I strongly advice to work in colab, or any other environment with a GPU available in order to minimise training time and being able to run multiple model training. \n",
        "Recall that experimenting is crucial.\n",
        "\n",
        "To check whether your instance has a GPU activated you can run the following code\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:    \n",
        "    raise SystemError('GPU device not found')\n",
        "```\n",
        "\n",
        "If you do not have the GPU enabled, just go to:\n",
        "\n",
        "`Edit -> Notebook Settings -> Hardware accelerator -> Set to GPU`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebYFI3su3fz"
      },
      "source": [
        "### Questions to answer\n",
        "\n",
        "1. Is the dataset balanced?\n",
        "2. What kind of preprocessing you think is necessary?\n",
        "3. Can you use some sort of transfer learning? Which one?\n",
        "4. How many items contains the word \"*bush*\"?\n",
        "5. How many items containing the word \"*pussy*\" are classified as \"positive\"?\n",
        "6. How many items are classified as \"neutral\" and do not contain the words \"phone\", \"computer\", \"President\" and \"suck\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVqk_vl8w6D4"
      },
      "source": [
        "## General assignements\n",
        "\n",
        "* Write your code following [PEP8 style guide](https://www.python.org/dev/peps/pep-0008/).\n",
        "* Docstrings has to be written in [Google Style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).\n",
        "* It is strongly adviced to write your modules to collect functions and import them in the notebook (this will make the following point almost effortless). To import custom modules in colab [look at this example](https://colab.research.google.com/drive/1uvHuizCBqFgvbCwEhK7FvU8JW0AfxgJw#scrollTo=psH0aLrvoh78).\n",
        "* Once you are sure the notebook runs smoothly, write a python script to be executed from a command line interpreter to train your model:\n",
        "\n",
        "```bash\n",
        "python3 -m train --conf config.yml\n",
        "```\n",
        "\n",
        "The `config.yml` file has to contain configuration instructions on model architecture (kind of layers, number of layers, number of units, activations, etc.), on training (number of epochs, size of batches, if apply early stopping, optimiser, etc.) and on script metadata (where to get data, where to save output model).\n",
        "\n",
        "* Finally (optionally), you can serve your model on a webpage thanks to tensorflow.js."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<div style=\"margin: 0 auto; text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/oscar-defelice/DeepLearning-lectures/blob/master/FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "</div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tza_pfKbyUv8"
      },
      "source": [
        "## Proposed Solution\n",
        "\n",
        "First of all, I will explore the dataset to see whether there are null values, etc. We already know a lot about the data thank to the readme of the dataset itself, so not a lot of work is required here. \n",
        "\n",
        "I choose pandas over tfds solution also to answer the preliminary questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4QEsNc8yW3u"
      },
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, Dropout, Activation, MaxPool1D, GlobalMaxPool1D, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0j2umEWy7s9"
      },
      "source": [
        "path = 'data/training.1600000.processed.noemoticon.csv'\n",
        "df = pd.read_csv(path, encoding='latin-1', names=['polarity', 'id', 'datetime', 'query', 'user_name', 'text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMtgehmizB8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e0c521be-f66d-4174-e916-66f5a662ede3"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>datetime</th>\n",
              "      <th>query</th>\n",
              "      <th>user_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity  ...                                               text\n",
              "0               0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1               0  ...  is upset that he can't update his Facebook by ...\n",
              "2               0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3               0  ...    my whole body feels itchy and like its on fire \n",
              "4               0  ...  @nationwideclass no, it's not behaving at all....\n",
              "...           ...  ...                                                ...\n",
              "1599995         4  ...  Just woke up. Having no school is the best fee...\n",
              "1599996         4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997         4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998         4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999         4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugo67x93zm-u"
      },
      "source": [
        "### Preliminary questions\n",
        "Here we propose answer to questions above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8mM0Zn-1Ucu"
      },
      "source": [
        "#### 1. Is the dataset balanced?\n",
        "\n",
        "To answer this question we have several possibilities, like counting values, plot an histogram, etc. Since I like fast answers, here I will make use of pandas `df.value_counts()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg54esPQzDiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "119db40d-e3ca-43e2-f425-f65896ff0f1a"
      },
      "source": [
        "df['polarity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sieHwmxj5Fj_"
      },
      "source": [
        "Hence, the dataset is perfectly balanced. Here another graphical solution: _i.e._ seaborn `counplot()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toh5cZk-5FKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "1011a78f-2db2-4d0f-fedb-4b0721717df6"
      },
      "source": [
        "sns.countplot(x=df.polarity, data=df);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA0btrKE1GbG"
      },
      "source": [
        "#### 2. What kind of preprocessing you think is necessary?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR-cGeTj1pME"
      },
      "source": [
        "This question admits many answers, for sure we need to clean and tokenise text. Since we want to get at least sentence structure, we do not want to simply delete `@username` or urls, we can create cleaning functions leveraging regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPkpNBDW0uIQ"
      },
      "source": [
        "def clean_regex(text, pattern, special_tkn):\n",
        "  \"\"\"clean_regex function to replace some regular expression with special token.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "          text to transform containing the regular expression.\n",
        "  \n",
        "  pattern: regex.Pattern or str\n",
        "            regex pattern (if passed as str, the function compile it in \n",
        "            regex.Pattern) to select the regular expression.\n",
        "  \n",
        "  special_tkn: str\n",
        "                special token to replace the regular expression.\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  str : the replaced text.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    return re.sub(pattern, special_tkn, text)\n",
        "  except ValueError:\n",
        "    pattern = re.compile(pattern)\n",
        "    return re.sub(pattern, special_tkn, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEbM2HNT6gmD"
      },
      "source": [
        "With this function, we only need to define our patterns and corresponding substitute strings, it might be useful doing that in a dictionary. This will be part of the constants of our script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz0Dq1I66s37"
      },
      "source": [
        "re_dict = {'user': ('@([A-Za-z0-9_])+', 'xxxusr'),\n",
        "           'url': ('(http|https)://([A-Za-z0-9_./%])+', 'xxxurl')}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JScO3zmq8X1H"
      },
      "source": [
        "We are now ready to define the proper cleaning function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o11Gvcw7oGZ"
      },
      "source": [
        "def clean_text(text, regex_patterns = re_dict, lower = True):\n",
        "  \"\"\"clean_text function.\n",
        "  The function we use to clean text in order to feed a later tokeniser.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "          the text to be transformed.\n",
        "\n",
        "  regex_patterns: dict of str: (str, str)\n",
        "                    a dictionary containg the name of the subsitution as key \n",
        "                    and the tuple (pattern, special_tkn) as values.\n",
        "  \n",
        "  lower: bool\n",
        "          boolean parameter to set whether we want to lower the text before \n",
        "          cleaning.\n",
        "  \"\"\"\n",
        "  if lower:\n",
        "    text = text.lower()\n",
        "  \n",
        "  for _, item in regex_patterns.items():\n",
        "    text = clean_regex(text, item[0], item[1])\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEP_hFxEAgFn"
      },
      "source": [
        "#### 3. Can you use some sort of transfer learning? Which one?\n",
        "\n",
        "Also here, we have several choices. The simplest one is to use a simple word-embedding like [gloVe](https://nlp.stanford.edu/projects/glove/) or a model available in [tfhub](https://tfhub.dev/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFxBzIBoBIX2"
      },
      "source": [
        "#### 4. How many items contains the word \"bush\"?\n",
        "\n",
        "This and the following questions are just matter of filter over pandas dataframe. I give one answer here and leave the rest as an exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCsS-KMSBG-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0e3666-ead7-434d-a960-e58487f9488e"
      },
      "source": [
        "df.text[df.text.str.contains('bush', case=False)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "939                @BillyBush she admitted to being a fake  \n",
              "2038       What is common between Chidambaram and George ...\n",
              "2045       What is common between Chidambaram and George ...\n",
              "4699       @emmabush Well, I didn't mention the stale Pee...\n",
              "5099       @airbush  thanks for the guru links, but they ...\n",
              "                                 ...                        \n",
              "1573628    @bravesgirl5 naw, just find the nearest bush a...\n",
              "1581019    @kbushling pho at 4 30 in the morning. Aahh. S...\n",
              "1583931    @nthlondonhippy Jed Bush being the proposed ob...\n",
              "1590437                               @meghanbushell Thanks \n",
              "1591967             Just woke up going to bush gardens soon \n",
              "Name: text, Length: 499, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aDS_tZ9BpmI"
      },
      "source": [
        "Thus we have $499$ elements in the dataframe containing \"bush\" before preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQR4wJSOB6oc"
      },
      "source": [
        "### The actual code\n",
        "\n",
        "As said, we want to write a proper-code solution, we already have some preprocessing function that can be stored in a python module called, for instance, `utils.preprocessing.py`\n",
        "\n",
        "Here we apply such functions and initially build our X, Y arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GgxD8lnDHsX"
      },
      "source": [
        "First, let's clean our text with previously defined functions. At the same time we buil the `y` vector as our polarity divided by $4$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxyrB1nGDN1Q"
      },
      "source": [
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "X = df['clean_text'].values\n",
        "y = df.polarity.values/4 # defined as an array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMRI-YWlEWq8"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGsivqBVDEWT"
      },
      "source": [
        "Hence, let's define and train a tokeniser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF5-oAwLCPdI"
      },
      "source": [
        "tokeniser = tf.keras.preprocessing.text.Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3-bzxpQFOZO"
      },
      "source": [
        "tokeniser.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XI-ewjmFg7d"
      },
      "source": [
        "Let's transform texts to sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka5ZWjN3FUwH"
      },
      "source": [
        "X_train = tokeniser.texts_to_sequences(X_train)\n",
        "X_test = tokeniser.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ex2GtPNzLt1"
      },
      "source": [
        "We are ready to pad sequences to `max_len`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "928ac25NydKc"
      },
      "source": [
        "#@title Tokeniser configuration\n",
        "\n",
        "max_len = 75#@param {type:\"integer\"}\n",
        "\n",
        "vocab_size = len(tokeniser.word_index) + 1 # plus one to take into account the padding token."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX7AYaKdyhRM"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQyZpC4AF8Hr"
      },
      "source": [
        "#### The model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huzhzfiUF_Hq"
      },
      "source": [
        "Here we can build a model to be trained on sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L75h9WHFnVf"
      },
      "source": [
        "#@title Model Parameters\n",
        "#@markdown Here we give a minimal set of parameters for model configuration.\n",
        "\n",
        "emb_dim = 64 #@param {type:\"integer\"}\n",
        "dropout_rate = 0.3#@param {type: \"number\"}\n",
        "\n",
        "learning_rate = 0.001#@param {type: \"number\"}\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Hu_jBK9igc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc46f15d-3c1a-4366-84dc-da5fddaa8249"
      },
      "source": [
        "# build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, output_dim = emb_dim, input_length=max_len))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(LSTM(15))\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss=loss, metrics=[metric], optimizer=opt)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 75, 64)            17805504  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 75, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 15)                4800      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               1600      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 17,812,005\n",
            "Trainable params: 17,812,005\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUj1lkYFCVwW"
      },
      "source": [
        "You can see how the Embedding layer contributes for the greatest number of weights.\n",
        "If you try and train the model like it is, the training time on GPU is around 30 minutes per epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNddfMmDHQz"
      },
      "source": [
        "##### Pre-trained Embedding layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RToXyl0Y8yMC"
      },
      "source": [
        "There are several possibilities: \n",
        "\n",
        "* GloVe embedding\n",
        "* You can use [TensorFlow Hub](https://tfhub.dev) available models. This might be a good choice but you lose some control over the text conversion into sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGL9solc-eoC"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnPPyYQT-iUY"
      },
      "source": [
        "We are now ready to train our model on data. First we create tensorflow datasets, tehn we call the fit method on our istanciated model.\n",
        "\n",
        "The training time on GPU is around 30 min per epoch.\n",
        "You can see how accuracy improves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI2NFYaW-rdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b0efe4-262b-4c14-e05d-e55133c50986"
      },
      "source": [
        "#@title Model Training\n",
        "#@markdown We can move the slider to set number of epochs \n",
        "#@markdown and give a different batch size.\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "number_of_epochs = 3 #@param {type: \"slider\", min: 1, max: 12}\n",
        "batch_size = 128 #@param [\"2\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\", \"512\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# train dataset\n",
        "ds_train_encoded = train_dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "# test dataset\n",
        "ds_test_encoded = test_dataset.batch(batch_size)\n",
        "\n",
        "model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 1985s 198ms/step - loss: 0.4153 - accuracy: 0.8096 - val_loss: 0.3871 - val_accuracy: 0.8243\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 1974s 197ms/step - loss: 0.3630 - accuracy: 0.8386 - val_loss: 0.3925 - val_accuracy: 0.8265\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 1971s 197ms/step - loss: 0.3332 - accuracy: 0.8546 - val_loss: 0.4007 - val_accuracy: 0.8254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff6e2bbd290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC4i6Nj9nOPp"
      },
      "source": [
        "As you can see the validation accuracy is over $80\\%$ as required."
      ]
    }
  ]
}